{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a34dfbf5",
   "metadata": {},
   "source": [
    "# Proyecto 1 MNIST\n",
    "\n",
    "Este notebook sigue los pasos establecidos en clase para la creación de un modelo de machine learning utilizando él data set MNIST\n",
    "\n",
    "Este consiste en 70,000 imagenes de numeros escritos a mano con su etiqueta del valor que representan\n",
    "\n",
    "![MNIST](./MNIST.png)\n",
    "\n",
    "1. Read Data (Leer los datos)\n",
    "2. Data Preprocessing (Preprocesamiento de los datos)\n",
    "3. Model Creation (Creación del modelo)\n",
    "4. Adjust Model with Historic Data (Ajustar el modelo con información histórica)\n",
    "5. Prediction from new Data (Predecir a partir de nueva información)\n",
    "6. Visualization of Results (Visualizar los resultados)\n",
    "\n",
    "\n",
    "Para correr este notebook es necesario tener: python3, anaconda y tensorflow.\n",
    "\n",
    "### Integrantes:\n",
    "\n",
    "* Gustavo Jose Hernandez Sotres\n",
    "* Alberto Sandoval Castro\n",
    "* Rafael Juárez Badillo Chávez\n",
    "* Diego Pintor Ochoa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "502e5c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-18 22:52:15.031924: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e85ef3",
   "metadata": {},
   "source": [
    "## Leer los datos\n",
    "\n",
    "Este es un paso sencillo, ya que las librerías que usamos contienen él data set en ellas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93842ebf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28), (10000, 28, 28))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data Import\n",
    "(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff9c6ee",
   "metadata": {},
   "source": [
    "Vemos como los datos están guardados en arreglos de 2 dimensiones de 28x28"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00b8262",
   "metadata": {},
   "source": [
    "## Preprocesamiento de datos\n",
    "\n",
    "en este caso solo preparamos la información para poder utilizarla después en SKLEARN que la necesita en un arreglo unidimensional de 784 (28x28) entradas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c3bcf73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Reshape, for SKLEARN usage\n",
    "Skdata_X = X_train.reshape(60000,784)\n",
    "Skdata_X_test = X_test.reshape(10000,784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7693abbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 784), (10000, 28, 28))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Skdata_X.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec1abe3",
   "metadata": {},
   "source": [
    "## Creación del modelo Y ajuste del modelo\n",
    "\n",
    "Para este dataset tenemos planeado utilizar 5 modelos\n",
    "\n",
    "#### Modelos a usar\n",
    "\n",
    "- Logistic Regression\n",
    "- SVM\n",
    "- Random Forest\n",
    "- Neural Network (MLP)\n",
    "- CNN (Deep Learning)\n",
    "\n",
    "En cada sección creamos el modelo y lo ajustamos (entrenamos) con la información de entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c061d51a",
   "metadata": {},
   "source": [
    "### Logistic Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8dbb2628",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gustavosotres/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model Creation\n",
    "# Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "mnist_logistic = LogisticRegression()\n",
    "mnist_logistic.fit(Skdata_X,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dcaacc0",
   "metadata": {},
   "source": [
    "### SVM - Support Vector Machines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a496cbcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gustavosotres/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                ('linearsvc', LinearSVC(random_state=0, tol=1e-05))])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SVM\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "mnist_svm = make_pipeline(StandardScaler(), LinearSVC(random_state=0, tol=1e-5))\n",
    "# Model Training\n",
    "mnist_svm.fit(Skdata_X,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2eaf5f",
   "metadata": {},
   "source": [
    "### Random Forest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7579946",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_estimators=10)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "mnist_randForest = RandomForestClassifier(n_estimators=10)\n",
    "mnist_randForest.fit(Skdata_X,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829f0040",
   "metadata": {},
   "source": [
    "### Neural Network - MLP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "097bb327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-18 23:01:44.944034: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 3s 1ms/step - loss: 2.5095 - accuracy: 0.8541 - val_loss: 0.4976 - val_accuracy: 0.8881\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.3910 - accuracy: 0.9048 - val_loss: 0.3733 - val_accuracy: 0.9101\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.2822 - accuracy: 0.9287 - val_loss: 0.3176 - val_accuracy: 0.9316\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.2515 - accuracy: 0.9385 - val_loss: 0.2858 - val_accuracy: 0.9342\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.2233 - accuracy: 0.9446 - val_loss: 0.2425 - val_accuracy: 0.9377\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.2142 - accuracy: 0.9469 - val_loss: 0.2694 - val_accuracy: 0.9401\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.1972 - accuracy: 0.9519 - val_loss: 0.3515 - val_accuracy: 0.9382\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.1935 - accuracy: 0.9531 - val_loss: 0.2710 - val_accuracy: 0.9471\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.1797 - accuracy: 0.9572 - val_loss: 0.3103 - val_accuracy: 0.9380\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.1828 - accuracy: 0.9569 - val_loss: 0.2856 - val_accuracy: 0.9503\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7b9d3b8f40>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NN (MLP)\n",
    "\n",
    "model_nn = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28,28)),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dense(10)\n",
    "])\n",
    "model_nn.compile(optimizer='adam',\n",
    "              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "model_nn.fit(X_train, y_train, epochs=10,\n",
    "         validation_data = (X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c264f3",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network - CNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "007c06fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 25s 13ms/step - loss: 0.2383 - accuracy: 0.9432 - val_loss: 0.0753 - val_accuracy: 0.9749\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 24s 13ms/step - loss: 0.0645 - accuracy: 0.9805 - val_loss: 0.0434 - val_accuracy: 0.9855\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.0503 - accuracy: 0.9847 - val_loss: 0.0389 - val_accuracy: 0.9874\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 24s 13ms/step - loss: 0.0420 - accuracy: 0.9870 - val_loss: 0.0585 - val_accuracy: 0.9817\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 25s 13ms/step - loss: 0.0380 - accuracy: 0.9884 - val_loss: 0.0533 - val_accuracy: 0.9844\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 24s 13ms/step - loss: 0.0311 - accuracy: 0.9901 - val_loss: 0.0470 - val_accuracy: 0.9867\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 24s 13ms/step - loss: 0.0272 - accuracy: 0.9918 - val_loss: 0.0499 - val_accuracy: 0.9877\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.0238 - accuracy: 0.9930 - val_loss: 0.0571 - val_accuracy: 0.9857\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.0233 - accuracy: 0.9932 - val_loss: 0.0648 - val_accuracy: 0.9886\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 24s 13ms/step - loss: 0.0195 - accuracy: 0.9943 - val_loss: 0.0539 - val_accuracy: 0.9884\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7b74d7c850>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CNN \n",
    "from tensorflow.keras import layers, models\n",
    "train_images = X_train.reshape(60000,28,28,1)\n",
    "test_images = X_test.reshape(10000,28,28,1)\n",
    "model_cnn = models.Sequential()\n",
    "model_cnn.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28,28,1)))\n",
    "model_cnn.add(layers.MaxPooling2D((2, 2)))\n",
    "model_cnn.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model_cnn.add(layers.MaxPooling2D((2, 2)))\n",
    "model_cnn.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model_cnn.add(layers.Flatten())\n",
    "model_cnn.add(layers.Dense(64, activation='relu'))\n",
    "model_cnn.add(layers.Dense(10))\n",
    "model_cnn.compile(optimizer='adam',\n",
    "              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "# Model Fitting\n",
    "model_cnn.fit(train_images, y_train, epochs=10, \n",
    "                    validation_data=(test_images, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4978b982",
   "metadata": {},
   "source": [
    "## Predecir con información nueva\n",
    "\n",
    "Es aquí donde la separación de nuestra información en: entrenamiento y test muestra su importancia, ya que si entrenáramos nuestro modelo con toda la información que tenemos el modelo siempre daría resultados positivos con ella, ya que sabe que información recibirá de entrada y como queremos probar como el modelo haría con información que nunca ha visto, entonces dividimos y entrenamos con una parte, para después probar el modelo con la otra parte, con información que nunca había visto y así tener una métrica más correcta de que tan efectivo es."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba55d933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 702us/step\n",
      "313/313 [==============================] - 1s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "# Data Prediction\n",
    "# Logistic Regression\n",
    "fy_predict_logistic = mnist_logistic.predict(Skdata_X_test)\n",
    "# Support Vector Machine\n",
    "fy_predict_svm = mnist_svm.predict(Skdata_X_test)\n",
    "# Random Forest\n",
    "fy_predict_randForest = mnist_randForest.predict(Skdata_X_test)\n",
    "# MLP\n",
    "predict_nn = model_nn.predict(X_test) # Array que contiene los arrays con la probabilidad de \n",
    "                                        #que los datos de entrada pertenezcan a un label\n",
    "fy_predict_mlp = [] # Lista vacía, aquí se almacenarán los labels con la probabilidad más alta\n",
    "for i in range(len(predict_nn)): #Para cada array en predict_nn\n",
    "    fy_predict_mlp.append(np.argmax(predict_nn[i])) # Encuentra la posición de la prob más alta\n",
    "                                                    # y almacénala en la lista\n",
    "        \n",
    "#CNN \n",
    "predict_cnn = model_cnn.predict(X_test)\n",
    "fy_predict_cnn = []\n",
    "for i in range(len(predict_cnn)):\n",
    "    fy_predict_cnn.append(np.argmax(predict_cnn[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1467fd3",
   "metadata": {},
   "source": [
    "## Visualización de resultados\n",
    "\n",
    "Ahora que entrenamos nuestros modelos y obtuvimos resultados, es importante saber leer estos resultados. Esto nos permitirá hacer un análisis más preciso y entender si nuestro modelo es bueno o malo y así ajustar algunos parámetros que podemos controlar de los modelos.\n",
    "\n",
    "Utilizaremos cross validation para obtener 4 métricas\n",
    "\n",
    "- Un valor de exactitud\n",
    "- Un valor de precisión\n",
    "- Un valor de recall\n",
    "- El valor F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d34ecca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance evaluation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7389715b",
   "metadata": {},
   "source": [
    "Aquí estamos comparando todos los modelos, uno contra otro en todos los metodos, ya que cada uno nos puede dar información extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24bc97c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = [accuracy_score(y_test, fy_predict_logistic),\n",
    "            accuracy_score(y_test, fy_predict_svm),\n",
    "            accuracy_score(y_test, fy_predict_randForest),\n",
    "           accuracy_score(y_test, fy_predict_mlp),\n",
    "           accuracy_score(y_test, fy_predict_cnn)]\n",
    "precision = [precision_score(y_test, fy_predict_logistic, average='macro'),\n",
    "             precision_score(y_test, fy_predict_svm, average='macro'),\n",
    "             precision_score(y_test, fy_predict_randForest, average='macro'),\n",
    "            precision_score(y_test, fy_predict_mlp, average='macro'),\n",
    "            precision_score(y_test, fy_predict_cnn, average='macro')]\n",
    "recall = [recall_score(y_test, fy_predict_logistic, average='micro'),\n",
    "         recall_score(y_test, fy_predict_svm, average='micro'),\n",
    "         recall_score(y_test, fy_predict_randForest, average='micro'),\n",
    "         recall_score(y_test, fy_predict_mlp, average='micro'),\n",
    "         recall_score(y_test, fy_predict_cnn, average='micro')]\n",
    "f1 = [f1_score(y_test, fy_predict_logistic, average = 'weighted'),\n",
    "     f1_score(y_test, fy_predict_svm, average = 'weighted'),\n",
    "     f1_score(y_test, fy_predict_randForest, average = 'weighted'),\n",
    "     f1_score(y_test, fy_predict_mlp, average = 'weighted'),\n",
    "     f1_score(y_test, fy_predict_cnn, average = 'weighted')]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e103a74e",
   "metadata": {},
   "source": [
    "Ahora vemos estos resultados para poder hacer los ajustes pertinentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a0fbe85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Logistic</th>\n",
       "      <th>SVM</th>\n",
       "      <th>RandForest</th>\n",
       "      <th>MLP</th>\n",
       "      <th>CNN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>0.925500</td>\n",
       "      <td>0.911900</td>\n",
       "      <td>0.945300</td>\n",
       "      <td>0.950300</td>\n",
       "      <td>0.988400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.924663</td>\n",
       "      <td>0.910922</td>\n",
       "      <td>0.944991</td>\n",
       "      <td>0.951026</td>\n",
       "      <td>0.988379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.925500</td>\n",
       "      <td>0.911900</td>\n",
       "      <td>0.945300</td>\n",
       "      <td>0.950300</td>\n",
       "      <td>0.988400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1</th>\n",
       "      <td>0.925383</td>\n",
       "      <td>0.911502</td>\n",
       "      <td>0.945197</td>\n",
       "      <td>0.950426</td>\n",
       "      <td>0.988389</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Logistic       SVM  RandForest       MLP       CNN\n",
       "Accuracy   0.925500  0.911900    0.945300  0.950300  0.988400\n",
       "Precision  0.924663  0.910922    0.944991  0.951026  0.988379\n",
       "Recall     0.925500  0.911900    0.945300  0.950300  0.988400\n",
       "F1         0.925383  0.911502    0.945197  0.950426  0.988389"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perf_metrics = pd.DataFrame(data = [accuracy,precision,recall,f1], columns = [\"Logistic\",\n",
    "                                                                              \"SVM\", \"RandForest\",\"MLP\",\"CNN\"],\n",
    "                           index = [\"Accuracy\",\"Precision\",\"Recall\",\"F1\"])\n",
    "perf_metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "3eb825599874f73d53a28eb1517ebf1ba28ce5d8e92d7a88138417e905ce08d5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
